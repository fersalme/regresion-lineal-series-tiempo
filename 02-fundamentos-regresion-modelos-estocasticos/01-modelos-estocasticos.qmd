---
title: "Fundamentos de Regresión y Modelos Estacionarios"
author: "Fernando Salcedo Mejía Eco, Ms"
afi: "Escuela de Transformación Digital - ETD | Programa de Ciencia de Datos"
date: "Febrero 2026"
format:
  revealjs:
    theme: [default, src/theme/style.scss]
    slide-number: true
    transition: fade
    scrollable: true
    code-fold: true
    embed-resources: true
    echo: true
    logo: src/img/logo_utb.png
    footer: "UTB-ETD | Programa de Ciencia de Datos"
title-slide-attributes:
    data-background-image: src/img/plantilla-inicio.png
    data-background-opacity: "0.6"
engine: jupyter
---

## Contenido del curso

::: {.callout-tip}
En esta unidad se abordan los principios de los modelos de regresión aplicados a las series temporales. Se introducen los
conceptos de estacionariedad, autocorrelación y autocovarianza, y cómo estos son fundamentales para trabajar con modelos de
series temporales.
:::
- Regresión simple y múltiple en el contexto de las series temporales
- Estacionariedad en series temporales
- Funciones de autocorrelación y autocovarianza
- Análisis de la autocorrelación y parcial autocorrelación en series temporales
- Modelos autoregresivos (AR) y medias móviles (MA)

## Regresión lineal en series de tiempo

::: {.callout-tip title = "Concepto clave"}
Podemos aplicar el concepto de regresión para pronosticar una variable $y_t$ asumiendo que tiene una relación esperada con otra en el tiempo $x_t$.
:::

$$
y_t = \beta_0 + \beta_1 x_{1, t} + \beta_2 x_{2, t} + \dots + \beta_k x_{k, t} + \varepsilon_t
$$

```{python}
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm

x = np.random.normal(50, 10, 100)
y = np.random.normal(2, 10, 100) + 2 * x + np.random.normal(0, 1, 100)
X = sm.add_constant(x)
lm = sm.OLS(y, X).fit()
y_hat = lm.predict()

plt.style.use('ggplot')
fig, ax = plt.subplots()
ax.scatter(y, x, color = "gray")
ax.plot(y_hat, x, color = "red")
plt.title('Regresión lineal')
plt.xlabel('X')
plt.ylabel('Y')
fig.show()
```

## Estimador Minimos Cuadrados Ordinarios (MCO)

::: {.callout-tip title="Definición"}
El estimador MCO permite buscar los coeficientes $\beta_k$ de una regresión tal que **minimiza la suma de los errores (el valor más pequeño)** de la regresión.
:::
$$
\varepsilon_t = y_t - \hat{y}_t \\
\hat{y}_t = \beta_0 + \beta_1 x_{1, t} + \beta_2 x_{2, t} + \dots + \beta_k x_{k, t} + \varepsilon_t
$$
$$
\sum_{t=1}^T \varepsilon_t^2 = \sum_{t=1}^T (y_t -
  \beta_{0} - \beta_{1} x_{1,t} - \beta_{2} x_{2,t} - \cdots - \beta_{k} x_{k,t})^2.
$$

- Supuesto $\hat{\varepsilon_{t}} \sim N(0, \sigma^2)$

## Gráfico conceptual del MCO

![](src/img/grafico_ols.png)

Referencia : Tomando de Green (2012) Econometric Analysis . 7 ed.

## Estimador MCO modelo lineal simple

- Para un modelo $Y_t = \beta_0 + \beta_1X_t$. El estimador por _MCO_ para $\beta_1$ sería

$$
\hat{\beta_1} = \frac{\sum(X_t - \overline{X})(Y_t - \overline{Y})}{\sum(X_t - \overline{X})^2} \\
\hat{\beta_0} = \overline{Y} - \hat{\beta_1}\overline{X}
$$

## Estimador MCO modelo general

- Tenemos el siguiente modelo lineal múltiple
$$
y_t = \beta_0 + \beta_1 x_{1, t} + \beta_2 x_{2, t} + \dots + \beta_k x_{k, t} + \varepsilon_t \\
\varepsilon_{t} \sim N(0, \sigma^2)
$$

- Para encontrar los $\hat{\beta}$ por _MCO_ conviene expresarlo en términos matriciales así :

$$
\left[
  \begin{matrix}
  y_1 \\
  y_2 \\
  \vdots \\
  y_T
  \end{matrix}
\right] = 
\left[
  \begin{matrix}
  1 & x_{1,1} & x_{2,1} & \dots & x_{k,1}\\
  1 & x_{1,2} & x_{2,2} & \dots & x_{k,2}\\
  \vdots& \vdots& \vdots&& \vdots\\
  1 & x_{1,T}& x_{2,T}& \dots& x_{k,T}
  \end{matrix}
\right] \times
\left[
  \begin{matrix}
  \beta_1 \\
  \beta_2 \\
  \vdots \\
  \beta_k
  \end{matrix}
\right] +
\left[
  \begin{matrix}
  \varepsilon_1 \\
  \varepsilon_2 \\
  \vdots \\
  \varepsilon_T
  \end{matrix}
\right] \\
$$

- Equivalente a :
$$
\mathbf{y} = \mathbf{X}\mathbf{\beta} + \mathbf{\varepsilon} \\
$$

- Donde las dimensiones del sistema $n$ filas, $k$ columnas :  $\mathbf{y} = n \times 1$, $\mathbf{X} = n \times k$, $\mathbf{\beta} = k \times 1$ y  $\mathbf{\varepsilon} = n \times 1$

 - Teniendo en cuenta que _MCO_ minimiza el error cuadrático en términos matriciales $\sum_{t=1}^T \varepsilon_t^2 = \mathbf{\varepsilon}'\mathbf{\varepsilon}$

$$
\mathbf{\varepsilon}'\mathbf{\varepsilon} = (\mathbf{y} - \mathbf{X}\mathbf{\beta})'(\mathbf{y} - \mathbf{X}\mathbf{\beta})
$$

- Expandiendo los términos tenemos :
$$
\mathbf{\varepsilon}'\mathbf{\varepsilon} = \mathbf{y}'\mathbf{y} - \mathbf{\beta}'\mathbf{X}'\mathbf{y} - \mathbf{y}'\mathbf{X}\mathbf{\beta} + \mathbf{\beta}'\mathbf{X}'\mathbf{X}\mathbf{\beta} \\
\mathbf{\varepsilon}'\mathbf{\varepsilon} = \mathbf{\varepsilon}'\mathbf{\varepsilon}  - 2\mathbf{y}'\mathbf{X}\mathbf{\beta} + \mathbf{\beta}'\mathbf{X}'\mathbf{X}\mathbf{\beta}
$$

- Minimizando la expresión tenemos :
$$
\frac{\partial \mathbf{\varepsilon}'\mathbf{\varepsilon}}{\partial \beta} = -2\mathbf{y}'\mathbf{X} + 2\mathbf{X}'\mathbf{X}\mathbf{\beta}  = 0 \\
$$

 - Resolviendo para $\beta$
$$
2\mathbf{X}'\mathbf{X}\mathbf{\beta} = 2\mathbf{y}'\mathbf{X} \\
\text{Sistema de ecuaciones normales} :
\mathbf{X}'\mathbf{X} ^ {-1} \mathbf{X}'\mathbf{X}\mathbf{\beta} = \mathbf{X}'\mathbf{X} ^ {-1} \mathbf{y}'\mathbf{X}
$$

- Finalmente, tenemos que el estimador _MCO_ es : 
$$
\mathbf{\hat{\beta}} = \mathbf{X}'\mathbf{X} ^ {-1} \mathbf{y}'\mathbf{X}
$$

::: {.callout-note title="Nota"}
- Si tenemos un sistema lineal de la forma $\mathbf{A}\mathbf{x} = \mathbf{b}$. La solución del sistema no homogéneo es $\mathbf{x} = \mathbf{A}^{-1} \mathbf{b}$ sí y solo sí $\mathbf{A}$ es una matriz invertible no singular.
:::

::: {.callout-note title="Nota"}
- Propiedad la transposición de matrices: 
- $(\mathbf{ABC})' = \mathbf{C'B'A'}$.
- Si $\mathbf{A}$ es simétrica entonces $\mathbf{A}'$ = $\mathbf{A}$
- Si $\mathbf{A}$ es inversible sí y solo sí $\mathbf{A}'\mathbf{A} = \mathbf{I}$
- Derivada de una matriz $a'x$, $\frac{\partial (a'x)}{\partial x} = a$
- Derivada de una matriz cuadrática, $x'Ax$ $\frac{\partial (x'Ax)}{\partial x} = 2Ax$
:::

## La interpretación de los $\beta$

- Si tanto $Y$ como $X$ son continuos
  $$\beta_k = \frac{\Delta Y}{\Delta X_k}$$
  $\beta_k$ = El cambio promedio esperado en $Y$ ante un cambio en $X$

- Si $Y$ es númerica y $X$ es dicotómica
  $$\beta_k = E(Y|X = 1) - E(Y|X = 0)$$
  $\beta_K$ = Es la diferencia promedio esperada en $Y$ cuando $X = 1$ respecto a $X = 0$

## Supuestos del modelo lineal

- Las observaciones provienen de una muestra aleatoria de una población
- **Independecia** : $Y_i, .. Y_n$ son independientes
- **Linealidad** : El modelo de regresión es lineal en los parámetros, aunque puede o no ser lineal en las variables. 
  - Lineal
  $$Y = \beta_0 + \beta_1 \log X$$
  - No lineal
  $$Y = \beta_0 + 1/\beta_1 X$$ 

- **Homocedasticidad** : El valor medio del error es igual a cero y La varianza del término de error, o de perturbación, es la misma sin importar el valor de X.
$$
\varepsilon_i \sim N(0, \sigma^2_{\varepsilon})
$$ 
- No hay autocorrelación entre las perturbaciones  
$$
Cov(\varepsilon_i, \varepsilon_j) = 0
$$
- **Exogeneidad** : Valores de X independientes del término de error $Cov(\varepsilon_i, \mathbf{X}) = 0$
- **Colinealiad perfecta** : no existe correlación perfecta entre las regresoras incluidas en el modelo de regresión $X_1 = 2X_2$
- **Rango completo** : El número de observaciones n debe ser mayor que el número de parámetros por estimar $N > X_k$

## Propiedades del estimador MCO

El supuesto de $\varepsilon \sim N(0, \sigma^2_{\varepsilon})$ tiene las siguientes implicaciones para los parámetros estimados por _MCO_

- **Son insesgados**
$$
E(\hat{\beta_k}) = \beta_k
$$

- **Tienen varianza mínima**. En combinación con 1, esto significa que son estimadores insesgados con varianza mínima, o eficientes.
$$
Var(\hat{\beta_0}) = \frac{\sum X^2_i}{n\sum(X_i - \overline{X})^2}, Var(\hat{\beta_1}) = \frac{\sigma^2}{\sum(X_i - \overline{X})^2}
$$

- **Presentan consistencia**; es decir, a medida que el tamaño de la muestra aumenta indefinidamente, los estimadores convergen hacia sus verdaderos valores.

## Ejercicio 1. Gráficar la convergencia de los parámetros estimados por MCO a sus verdaderos valores.

- Supongamos que los valores de $y$ siguen esta función lineal simple
$$
y = 12 + 20 x + \varepsilon
$$

- Donde $\beta_0 = 12$, $\beta_1 = 20$, $X \sim N(12, 24)$ y $\varepsilon \sim N(0, 1)$
- Simular la convergencia de los parámetros estimados por _MCO_ según incrementa la muestra.

```{python}
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm

n_sim = np.linspace(10, 10000, 100).astype(int)
true_beta = {"b0" : 12, "b1" : 20}
x_stats = {"mu" : 12, "sigma" : 24}
beta_hat = {"b0" : [], "b1" : []}

# Generar simulacion
for n in n_sim:
  np.random.seed(42 + n)
  x = np.random.normal(x_stats['mu'], x_stats['sigma'], n)
  y = true_beta['b0'] + true_beta['b1'] * x + np.random.normal(0, 1, n)
  X = sm.add_constant(x)
  lm = sm.OLS(y, X).fit()
  beta_hat['b0'].append(lm.params[0])
  beta_hat['b1'].append(lm.params[1])

plt.style.use('ggplot')
fig, axes = plt.subplots(1, 2, figsize=(12, 5))
colors = ['#d95f02', '#1b9e77']

# Iteramos sobre los resultados para graficar
for i, (key, value) in enumerate(beta_hat.items()):
    axes[i].plot(n_sim, value, label=f'Estimado {key}', color=colors[i], marker='o')
    axes[i].axhline(true_beta[key], color='black', linestyle='--', alpha=0.6, label=f'Real {key}')    
    axes[i].set_title(f'Convergencia de {key}')
    axes[i].set_xlabel('Tamaño de muestra (n)')
    axes[i].set_ylabel('Valor del parámetro')
    axes[i].legend()

plt.tight_layout()
plt.show()
```

## Medición del ajuste global del modelo lineal

- La bondad del ajuste mide cuán “bien” se ajusta la línea de regresión a los datos. Se calcula en el coeficiente de determinación $R^2$.
- Esta mide la proporción o el porcentaje de la variación total en $Y$ explicada por el modelo de regresión. 
$$
R^2 = \frac{\sum(\hat{y}_{t} - \bar{y})^2}{\sum(y_{t}-\bar{y})^2}
$$

- Sus valores $R^2 > 0$ en el rango $0 \leq R \leq 1$

## Significancia de los parámetros

- Los parámetros estimados por _MCO_ se le verifica su significancia a través de una prueba de hipótesis

$$
H_0: \beta_j = 0
$$
$$
H_a: \beta_j \neq 0
$$

$$
t_{1 - \alpha/2; (n - k)} = \frac{\beta_j - \hat{\beta_j}}{\sqrt{Var(\hat{\beta_j})}}
$$
- **Regla de descisión :** Si $P(t_{1 - \alpha/2; (n - k)}) < 0.05$ consideramos que hay evidencia suficiente contra $H_0$, por lo que se acepta $H_a$.
$$
\text{IC95 \%} =  \hat{\beta_j} \pm t_{1 - \alpha/2; (n - k)} \sqrt{Var(\hat{\beta_j})}
$$

## Pronostico e intervalos de confianza
- El pronostico del modelo estimado no es más que usar los $\hat{\beta}$ encontrados por _MCO_ sobre una nueva observación de $X$
$$
\hat{y_t} = \hat\beta_{0} + \hat\beta_{1} x_{1,t} + \hat\beta_{2} x_{2,t} + \cdots + \hat\beta_{k} x_{k,t},
$$

- Además un intervalo de confianza al 95% para le pronostico (asumiendo normalidad en los errores del modelo) se define como :
$$
\hat{y} \pm 1.96 \hat{\sigma}_e\sqrt{1+\frac{1}{T}+\frac{(x-\bar{x})^2}{(T-1)s_x^2}},
$$

::: {.callout-note title="Pronostico ex-antes y ex-post"}
- **Pronostico ex-antes :** Son aquellos que se realizan únicamente con los datos que están disponibles previamente de los predictores del modelo.
- **Pronostico ex-post :** Son aquellos que se realizan utilizando información posterior o nueva sobre los predictores del modelo.
:::

## Ejemplo 2 : El modelo macroeconómico del crecimiento

- El Producto Interno Bruto (PIB) es la agregación del

## Predictores temporales

## Evaluando el modelo MCO

## Ejercicio 3 : 

## Ejercicio 4 :